---
title: "Numeracy, Modelling and Data Management"
author: "Mike Spencer"
date: "27/03/2015"
output:
  html_document:
    highlight: tango
    theme: united
    toc: yes
---

## Introduction

Welcome to the final session of the doctoral training course: numeracy, modelling and data management. This course was originally scheduled to cover [GDAL](http://www.gdal.org/), and it will, but as it has been postponed by a week we've decided to include a general wrap up of the course and show some example work flows.

### This document

This document has been written using [R markdown](http://rmarkdown.rstudio.com/), which I understand Mark Naylor showed you. R markdown is similar to [Python notebooks](http://ipython.org/notebook.html), both of which allow you to write documents and embed code and results within them. R markdown can write output to html, pdf or word formats. R markdown works best for me as an electronic lab notebook, so all my analysis happens within R markdown and I write comments on what I find as I go along. This way, when you're writing that Nature paper and come back to find the code that generated your plot, it's all available within the same document. Your hunting will be vastly reduced!

Writing in markdown is really simple, plain text is just plain text. Here's a handy cheat sheet for headings and highlighting: http://blog.rstudio.org/2014/08/01/the-r-markdown-cheat-sheet/. If you want to do something more complicated it will happily accept html code (if that's what you're wrapping it to) or LaTeX. The easiest way to write in R markdown is using the R IDE [RStudio](http://www.rstudio.com/products/rstudio/download/), this is freely available for Linux, Windows and Mac. It's also installed on the school server, burn, open it by typing *rstudio* in a terminal.

This R markdown html is hosted as a github page, you can view the repository here: https://github.com/mikerspencer/training/tree/gh-pages which also includes the raw markdown file (Rmd) this webpage is based on.

I'm going to cover a few things in this session and talk about specific tools, these are the ones I use. They're certainly not the only ones available, but they are what I know. If you'd like to talk about other options, advantages and disadvantages drop by and see me in the Grant (rm 400) or [email](http://www.ed.ac.uk/schools-departments/geosciences/people?indv=2920&cw_xml=student.html). So here's what I'll cover:

* File structure
* Backups (rsync)
* Version control (git)
* Command line (bash)
* Programming (R)
* Spatial data
      + GIS (QGIS, GRASS)
      + GDAL (R, GIS)
      + Programming (R)
* Databases (SQLite)
* Writing (markdown & LaTeX)
* Example workflow

The key, for me, from these tools is that they help to make your research repeatable. This is really important holistically, but even on a personal level; if you need to make a few changes to a graph, table, paper... then all your analysis can be repeated with the click of a button - you don't have to remember it all in your head!


## File structure

I'm not going to say much about this, but do think about it. Having a consistent structure means you always know where to look for something. I wrote some ideas on my blog: https://scottishsnow.wordpress.com/2014/08/28/electronic-organisation-of-projects/


## Backups

Backing up your work is crucial. If you lose everything pity from your colleagues will not help get it back. Hopefully most of you will be working on the school server (adder/m drive), this is backed up. Another option could be that you work on your desktop and backup to the work server. If you do this copying folders across is so 1995. Use backup software like [rsync](http://en.wikipedia.org/wiki/Rsync) which scans for changes in files and only copies over what is needed. It's very fast. Here's an example that you would run in a bash terminal:

```{r backup, engine="bash", eval=F}
sudo rsync --delete --exclude='/.*/' --exclude='/dir/temp/' -azv -e ssh /home/s1066252 s1066252@adder.geos.ed.ac.uk:/home/s1066252/
```

The above example doesn't just work within the school, you can also use it at home or wherever. Remember to change the student number to your own!


## Version control

When I started using Git it totally revolutionised the way I worked. When I changed documents or code I no longer had to remember to make a copy before a major revision, or hold the previous version in my head when I made a minor revision. Each previous version was managed with a tag saying what I'd done to it. My mind was free to concentrate on what I wanted to achieve, not on avoiding breaking what I had (and not being able to salvage it).

Anything you write in plain text and change should be managed in version control. Most importantly this includes programming code and LaTeX documents. A tip for the latter is put each sentence on a new line, it will make it easier to track what changes. So long as you don't leave empty lines LaTeX will wrap these all to the same paragraph.

There are a few version control software programmes out there. I use Git, it's very widely used and there are a couple of great web hosting facilities for your repositories. The most well known of these is [GitHub](https://github.com/), another is [Bitbucket](https://bitbucket.org/). I think Bitbucket has a better documented and laid out web interface, but this is likely personal preference. The real difference between the two is what you get for free. GitHub allows you to have unlimited collaborators and host webpages (like this one), but charges for private repositories. Bitbucket has free private repositories, but charges for more than 10 people in a team, although it seems this is waived if you sign up with your academic email address. I use GitHub and Bitbucket for different things, I mainly use GitHub for public work and Bitbucket for my research.

You've had a dedicated session on Git, so I wont cover it in much detail here. I will say this is a great resource: http://rogerdudler.github.io/git-guide/, and the code below is what I use 99% of the time (anything else I have to look up):

```{r git, engine="bash", eval=F}
# -----------------------------------
# Setting up
# -----------------------------------
# From scratch
# Initialise your folder
git init
# Add the remote repo
git add remote ...

# Existing project, new computer
git clone ...

# -----------------------------------
# Day to day
# -----------------------------------
# Change directory
cd ~/working/directory/repo
# Track files
git add --all
# Commit with message
git commit -m "what I did"
# Send to remote repo
git push
```


## Command line


If you're working with lots of files or big ones the terminal is your friend. You've already used it for managing your version control repos and running backups, but it can do so much more. Here are some examples you might not have covered previously:

```{r bash, engine="bash", eval=F}
# Download data from a remote ftp
# This takes files that match the h17v0 string, but that have the number 2 or 3 afterwards.
# Actual examples for downloading MODIS snow cover satellite images for Scotland.
lftp -c 'open -e "mirror -I *h17v0[23]*hdf . ." ftp://n5eil01u.ecs.nsidc.org/SAN/MOSA/MYD10A1.005/'

# New directory
mkdir test

# List files in a directory
ls

# Show the first few files of a directory (useful if it has many files)
ls | head
```

You can also write programmes to do repetitive tasks, like resizing images (the below relies on ImageMagick):

```{r bash images, engine="bash", eval=F}
# -----------------------------------
# Save the below in a file called resizer.sh in a folder of pictures you want to make smaller
# -----------------------------------

mkdir ./small
for file in ./*
do convert $file -define jpeg:extent=0.3MB ./small/$file
done

# Comments
# Line 1 creates a sub-directory called small
# Line 2 lists all the files in the working directory (make sure it only has jpgs)
# Line 3 makes a copy of Line 2 files in small that are less than 0.5 MB in size
# Line 4 stops the script


# -----------------------------------
# Run these
# -----------------------------------
cd ~/Picture/folder
sh resizer.sh
```

I use bash to manage my Linux computers, so I can reinstall all programmes into a fresh build with the minimum of hassle. There's an example script here: https://scottishsnow.wordpress.com/software/

## Programming

You've seen a little programming in the section on bulk resizing images. Programming really is key to making your life easier. Any time you do the same thing more than a couple of times, you should probably write a script for it. This is for a number of reasons:

* It makes your work repeatable by someone else
* It reduces your chance of making a mistake
* Datasets are getting bigger, the tools we use to analyse them need to improve to cope with the volume and complexity

Most importantly...

* It develops your skills and is far more interesting than repeating the same task over and over!

Really, who wants to spend a week copy and pasting in a spreadsheet?

My preferred tool is R, purely because it's the first language I was exposed to. Turns out it's really good for data analysis, statistics and visualisation. There are a lot of folk in the School who use Python, which is particularly strong at being part of a bigger workflow. You will find people who use Matlab, I don't recommend this. I'm sure it's great, but the downside is it's expensive; while the University of Edinburgh has a licence, everywhere you work may not. Personally, I'd rather invest time in a tool I know I can take with me. R and Python are both [open source](http://en.wikipedia.org/wiki/Open-source_software), which means there are no restrictions on their use. However, you may not get a choice of programming language if you adopt code from elsewhere.

Good places to learn to code and get help are:

* [stackoverflow](http://stackoverflow.com/)
* [Coursera](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage)
* [Code Academy](http://www.codecademy.com/)
* [Software Carpentry](http://software-carpentry.org/)
* Blogs
      + [R bloggers](http://www.r-bloggers.com/)
      + [Volcan01010](http://all-geo.org/volcan01010/every-post-ever/)
      + [scottishsnow](https://scottishsnow.wordpress.com/tag/r/)
* Asking your peers!

## Spatial data

Spatial data has a big impact on lots of us working with environmental data, be that the location of our geology cross section, the distribution of snow cover in a catchment or where we conducted our social science questionnaires.

You should already have covered GIS, but you can also work with spatial data from the command line or in your programming language of choice. As this workshop was originally targeted at GDAL I'll also cover that tool

### GIS

The big name in GIS is ESRI who produce ArcGIS/MAP. It's a good thing to have on your cv, but it is prone to driving you mad with slow processing times and instability. For the same reasons as why I use open source programming tools, I also use open source GIS: [QGIS](http://qgis.org) and [GRASS](http://grass.osgeo.org/). QGIS is a great desktop GIS that interfaces with most databases, it has a strong cartographic ability so you can make good looking maps. For heavy lifting I like to use GRASS, its capability for processing raster (grid) datasets is formidable, but there is a learning curve involved.

The key is to get family with the language and concepts of GIS, don't get bogged down in learning which buttons to press.

### GDAL

GDAL is a suit of tools for translating spatial data. Most GIS software uses it (e.g. Arc, QGIS and GRASS do, MapInfo does not). It is written in Python so you can call it natively from there, there are also packages for R to use it with that.

The example below use R, but it would not take long to convert these to another language. This example:

* Converts file types (hdf to tif) (gdal_translate)
* Merges pairs of tiles together (gdal_merge)
* Reprojects merged tif to local projection (gdalwarp).

The example below I developed to convert MODIS snow cover hdf files to tifs and merge the pairs of tiles together, finally converting them to OSGB grid. It loops through 14 years of daily observations by two satellites, processing ~ 
18'000 raster images in total.

```{r gdal, eval=F}
# ---------------------------------------
# Raster merge script
# ---------------------------------------
 
# ---------------------------------------
# Set up
# ---------------------------------------
setwd("~/dir/MODIS/data/")
install.packages(c("rgdal", "raster", "gdalUtils"))
library(rgdal)
library(gdalUtils)
library(raster)
 
# ---------------------------------------
# Script
# ---------------------------------------
 
# Find all directories
d = list.dirs(".", recursive=T, full.names=T)
d = d[nchar(d)>10]
 
# Define subject area in target projection
coord = c(048900, 527000, 475000, 1260953.928)
 
# Write hdf to single OSGB tif
# Set up a timer so you can estimate how long it will take (based on a subset)
ptm = proc.time()
# Loop through directories
lapply(d, function(i){
   # Progress indicator
   print(i)
   # File list
   f = list.files(i, full.names=T, recursive=T)
   # Check folder has enough files
   if(length(f)==2){
   # Define new file name
   n = paste0(substr(f[1], 1, 18), substr(f[1], 28, 34))
   # Process each folder
   for(j in 1:2){
      # Convert to tif
      # use gdal_info to check which hdf layer you need and adjust sd_index
      gdal_translate(f[j], paste0(n, j, ".tif"), sd_index=1)
   }
   # Merge
   system(paste0("gdal_merge.py ", paste0(n, 1, ".tif"), " ", paste0(n, 2, ".tif"), " -o ", paste0(n, ".tif")))
   # Reproject to OSGB and clip to subject area
   gdalwarp(paste0(n, ".tif"), paste0(n, "OSGB", ".tif"), s_srs="+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs", t_srs="EPSG:27700", srcnodata=-3000, dstnodata=-3000, te=coord)
   # Delete mid step tifs
   f = list.files(i, full.names=T, recursive=T, pattern="tif")
   # Vary this depending on your directory length and choice of file names
   f = f[nchar(f)<31]
   file.remove(f)
   } else {
      print("no data")
   }
})
proc.time() - ptm
rm(ptm)
```

### Programming

QGIS and GRASS (even ArcGIS) can be controlled with Python, and have a built in Python console. GRASS is fully fledged in the terminal, so your bash skills can come in handy! Both can also interact with R, QGIS via a plugin and GRASS via the R package *spgrass6*. I've written about using GRASS and R together here: https://scottishsnow.wordpress.com/2014/08/24/many-rastered-beast/.

You can also use R to undertake GIS like tasks, this gives you the advantage of having your data all ready for statistics, the drawback is that R reads data into RAM, so be careful how big the files are you open! I've written about using R for spatial work here: https://scottishsnow.wordpress.com/2015/02/24/uk-rare-snowmelt-estimation/

## Databases

I store my primary dataset, the [Snow Survey of Great Britain](https://scottishsnow.wordpress.com/2014/04/09/snow-survey-of-great-britain/) in a small database. It took me ages to get around to doing it, but it's been really useful and I'm now able to do far more work than I could before. I recommend using a database if your data are complex, e.g. time series and or sites. Write a little script to write your incoming data to a database, then it's one task that's a lot easier each month (if that's your sampling period). I'm doing some work for a PhD student in Crew on this at the moment, so please speak to me if you need any pointers.

Have a look at using SQLite, there are packages for Python and R and with an extension you can link it to a GIS. You can even get a [plugin for Firefox](https://addons.mozilla.org/en-Us/firefox/addon/sqlite-manager/) to query your data. Have a look at the example workflow at the bottom of this document to see one in action.

## Writing

Science isn't complete until you communicate it. As the process evolves, there are many different mediums available to us (blogs, papers, magazines...), but as PhD students we will always have to produce a thesis! Having written using Word for many years of undergrad and employment, it's been a breath of fresh air using [LaTeX](en.wikipedia.org/wiki/LaTeX) for my written work since beginning my PhD. It takes the stress out of structuring your documents and is much quicker to write as you're not distracted by how it looks. There's a bit of a learning curve, but it's well worth it.

As I mentioned earlier, try and complete your data analysis in R markdown or Python notebooks. It will be a lot easier when you review what you've done.

Consider keeping a blog, it helps you write and it's a good place to record things when you figure out something new. In the bigger picture they're a good place to advertise papers as well!

## Example workflow

Finally, I'm going to point you to an example workflow. This is something I did as part of my blog for science outreach. It:

* Takes monthly weather data from the Met Office website
* Rearranges the files
* Writes to a database
* Makes some exploratory analysis plots
* Generates correlations

You can view the results here: [UK Sept weather improving?](https://scottishsnow.wordpress.com/2014/09/19/sept-weather/), [Scottish winter weather](https://scottishsnow.wordpress.com/2014/09/19/scottish-winter-weather/) and [Climate trends in Scottish seasons](https://scottishsnow.wordpress.com/2015/03/20/tapply-on-steroids-by-climate-trends-in-scottish-seasons/). Most importantly, the code to do all the above is available on [Bitbucket](https://bitbucket.org/mikerspencer/uk_climate). So get stuck in and start cloning!